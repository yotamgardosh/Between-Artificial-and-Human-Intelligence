{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yotamgardosh/Between-Artificial-and-Human-Intelligence/blob/main/language_acquisition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings and Language Acquisition\n",
        "\n",
        "In this workshop you will get a chance to experiment with both static  (word2vec/GloVe) and contextual (self-attention) word embeddings.\n",
        "To do so, you will train your own GPT2 from scratch on \"Alice's Adventures in Wonderland\".\n",
        "\n",
        "Along the way you will (hopefully) learn to:\n",
        "1. Run code in colab\n",
        "2. Work with python libraries useful for NLP and ML in general (gensim, pytorch, numpy, etc.)\n",
        "2. Use code from outside [git repo](https://github.com/karpathy/minGPT.git)\n",
        "3. Build a training dataset from [raw text](https://archive.org/stream/alicesadventures19033gut/19033.txt)\n",
        "4. Train a Transformer based Language Model from scratch!\n",
        "5. Generate text using your LM ðŸ˜€\n",
        "6. Extract and plot self-attention scores from your model.\n"
      ],
      "metadata": {
        "id": "TAf83FhnDZMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a feeling for static embedding arithmetic you can play around with the following demo:\n",
        "\n",
        "https://dash.gallery/dash-word-arithmetic/\n",
        "\n",
        "(These are word2vec embedding trained on the Google News Dataset.)\n",
        "\n"
      ],
      "metadata": {
        "id": "J-IAqkQtABb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see how easy it is to implement in code you can try out the [gensim](https://radimrehurek.com/gensim/models/keyedvectors.html) library, and check if the same effets reproduce with GloVe embeddings.\n",
        "\n",
        "The follwoing functions may be helfpul: most_similar and doesnt_match."
      ],
      "metadata": {
        "id": "S_w_WTTzAg42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n"
      ],
      "metadata": {
        "id": "YMw3RKxuaZaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = word_vectors.most_similar(positive=['hitler','italy'], negative=['germany'])\n",
        "most_similar_key, similarity = res[0]\n",
        "print(f\"{most_similar_key}: {similarity:.4f}\")\n",
        "\n",
        "res = word_vectors.similar_by_word('israel')\n",
        "print(res)\n",
        "\n",
        "print(word_vectors.doesnt_match([\"football\", \"tenis\", \"judo\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBWD1BXwLY9C",
        "outputId": "e6ce3ea5-fff0-49f3-de71-214b2a1efaa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mussolini: 0.8272\n",
            "[('israeli', 0.8549681901931763), ('palestinians', 0.8094196915626526), ('palestinian', 0.7847806811332703), ('lebanon', 0.7811506390571594), ('syria', 0.7781012654304504), ('israelis', 0.7683233022689819), ('jerusalem', 0.766927182674408), ('gaza', 0.7554308772087097), ('netanyahu', 0.732354462146759), ('arafat', 0.731320321559906)]\n",
            "tenis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "It's time to train your very own GPT2!\n",
        "\n",
        "First, you need to clone the repo and set up the environment."
      ],
      "metadata": {
        "id": "5dUH_3BlDj6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/karpathy/minGPT.git\n",
        "!pip install -e 'minGPT/'"
      ],
      "metadata": {
        "id": "7kG6WP-yvsjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae7426d-3fff-4761-c93a-2ec979ed4158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'minGPT'...\n",
            "remote: Enumerating objects: 489, done.\u001b[K\n",
            "remote: Total 489 (delta 0), reused 0 (delta 0), pack-reused 489\u001b[K\n",
            "Receiving objects: 100% (489/489), 1.44 MiB | 5.06 MiB/s, done.\n",
            "Resolving deltas: 100% (260/260), done.\n",
            "Obtaining file:///content/minGPT\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from minGPT==0.0.1) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->minGPT==0.0.1) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->minGPT==0.0.1) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->minGPT==0.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->minGPT==0.0.1) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->minGPT==0.0.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->minGPT==0.0.1) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->minGPT==0.0.1)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->minGPT==0.0.1)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->minGPT==0.0.1)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->minGPT==0.0.1)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->minGPT==0.0.1)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->minGPT==0.0.1)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->minGPT==0.0.1)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->minGPT==0.0.1)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->minGPT==0.0.1)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->minGPT==0.0.1)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->minGPT==0.0.1)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->minGPT==0.0.1) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->minGPT==0.0.1)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->minGPT==0.0.1) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->minGPT==0.0.1) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, minGPT\n",
            "  Running setup.py develop for minGPT\n",
            "Successfully installed minGPT-0.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After cloning, go to Runtime and \"Restart Session\".\n",
        "Then run the next cell."
      ],
      "metadata": {
        "id": "UP0OnDYxvvOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, './minGPT/')\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from mingpt.bpe import get_encoder, BPETokenizer"
      ],
      "metadata": {
        "id": "Sp9biSquDzAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some consts you'll need for your training.\n",
        "\n",
        "Feel free to experiment with the different hyper-parameters, but be aware that increasing the BLOCK_SIZE or BATCH_SIZE too much may lead to your colab running out of memory.\n",
        "\n",
        "Don't forget to upload the text file of \"Alice's Adventures in Wonderland\" to colab, and update the ALICE_IN_WONDERLAND_PATH const if necessary."
      ],
      "metadata": {
        "id": "aPkKT-IbD-Bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ALICE_IN_WONDERLAND_PATH = 'alice_in_wonderland.txt'\n",
        "BLOCK_SIZE = 32\n",
        "VOCAB_SIZE = 50257\n",
        "NUM_BLOCKS_IN_GPT = 12\n",
        "MODEL_TYPE = 'gpt2'\n",
        "MAX_ITERS = 2000\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 5e-4\n",
        "NUM_ATTN_HEADS = 12\n"
      ],
      "metadata": {
        "id": "ZVmLfpN5EX_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to create a training dataset out of the raw text of \"Alice's Adventures in Wonderland\".\n",
        "\n",
        "FIrst, read the text from the file into a string.\n",
        "Then encode the string using the supplied Byte Pair Encoder.\n",
        "\n",
        "In the file [bpe.py](https://github.com/karpathy/minGPT/blob/master/mingpt/bpe.py), you can find an example for how to use the encoder.\n",
        "\n",
        "Instead of encoder.encode_and_show_work() use encoder.encode().\n",
        "This function will return a list of token indexes corresponding to the text.\n"
      ],
      "metadata": {
        "id": "zm1LlXb7E7x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open (ALICE_IN_WONDERLAND_PATH, 'r') as f:\n",
        "  Alice_text = f.read()\n",
        "\n",
        "encoder = get_encoder()\n",
        "BP_idx = encoder.encode(Alice_text)"
      ],
      "metadata": {
        "id": "R_8qcN-IiGf3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0aa0e3-4ca5-4688-820f-7e8c205f9469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading https://openaipublic.blob.core.windows.net/gpt-2/models/124M/encoder.json to /root/.cache/mingpt/encoder.json\n",
            "downloading https://openaipublic.blob.core.windows.net/gpt-2/models/124M/vocab.bpe to /root/.cache/mingpt/vocab.bpe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, you should write a custom dataset class that gets the token indexes from the encoder as input, inherets from [Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files), and  implements three functions: \\_\\_init__, \\_\\_len__, and \\_\\_getitem__.\n",
        "\n",
        "The \\_\\_init__ function just needs to save the token indexes.\n",
        "\n",
        "The \\_\\_len__ function returns the number of samples in our dataset (the largest i for which __getitem__ returns a valid sample).\n",
        "\n",
        "The \\_\\_getitem__ function loads and returns a sample (input, label) from the dataset at the given index i.\n",
        "\\_getitem_ should return  2 objects of type torch.tensor with dtype=torch.long"
      ],
      "metadata": {
        "id": "Vtom67ZUC2fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that GPT2 is an Auto-Regressive text model so we train the model to predict the next token based on the given prefix.\n",
        "Meaning that the i-th **input** will be the next block_size tokens up to i, tokens[i : i + block_size] and the **label** will be\n",
        "tokens[i + 1 : i + block_size + 1]."
      ],
      "metadata": {
        "id": "71-Xm6EIkz37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AIWDataset(Dataset):\n",
        "\n",
        "    def __init__(self, token_idxs):\n",
        "        self.token_idxs = token_idxs;\n",
        "\n",
        "    def __len__(self):\n",
        "        # ensures that there are enough tokens for each input sequence\n",
        "        return len(self.token_idxs) - (BLOCK_SIZE + 1) # + 1 accounts for overlap between input and output\n",
        "\n",
        "    def __getitem__(self, i): # return x,y for training\n",
        "        # the tokens from i to i + Block size, model predicts until i + BLOCK_SIZE + 1\n",
        "        x = torch.tensor(self.token_idxs[i : i + BLOCK_SIZE], dtype = torch.long)\n",
        "        # the tokens from i + 1 to i + Block size + 1 for testing the prediction\n",
        "        y = torch.tensor(self.token_idxs[i+1 : i + BLOCK_SIZE + 1], dtype = torch.long)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "dataset = AIWDataset(BP_idx)"
      ],
      "metadata": {
        "id": "OHohcN9BamX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now all that's left is to load and train you model.\n",
        "Check out the README of the minGPT repo for [instructions](https://github.com/karpathy/minGPT/blob/master/README.md).\n",
        "\n",
        "Use the constants provided above to set the values in model_config (model_type, block_size, vocab_size).\n",
        "\n",
        "You can add a callback function to your trainer to see how the loss changes during training.\n",
        "Use the constants provided above to set the values in trainer_config (learning_rate, max_iters, batch_size).\n",
        "\n",
        "If you haven't changed the hyper-parmeters, training on T4 machine should take ~10m.\n",
        "\n",
        "If you get out of memory error, try using a smaller version of the model for example 'gpt-mini'"
      ],
      "metadata": {
        "id": "ELJOPYyuGDK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mingpt.model import GPT\n",
        "from mingpt.trainer import Trainer\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    \"\"\"\n",
        "    Load a pre-configured GPT model.\n",
        "\n",
        "    Returns:\n",
        "        model (GPT): The loaded GPT model.\n",
        "    \"\"\"\n",
        "    # Get default configuration for the GPT model\n",
        "    model_config = GPT.get_default_config()\n",
        "    # Set the model type (e.g., \"GPT2\" or \"GPT3\")\n",
        "    model_config.model_type = MODEL_TYPE\n",
        "    # Set the vocabulary size based on the OpenAI model's vocabulary\n",
        "    model_config.vocab_size = VOCAB_SIZE\n",
        "    # Set the block size, representing the input context length for the model\n",
        "    model_config.block_size = BLOCK_SIZE\n",
        "    # Initialize and return the GPT model with the specified configuration\n",
        "    model = GPT(model_config)\n",
        "    return model\n",
        "\n",
        "def batch_end_callback(trainer):\n",
        "    \"\"\"\n",
        "    Callback function called at the end of each training batch.\n",
        "\n",
        "    Args:\n",
        "        trainer (Trainer): The trainer object managing the training process.\n",
        "    \"\"\"\n",
        "    # Print training progress every 100 iterations\n",
        "    if trainer.iter_num % 100 == 0:\n",
        "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "\n",
        "def train_model(model, train_dataset):\n",
        "    \"\"\"\n",
        "    Train the specified model using the provided dataset.\n",
        "\n",
        "    Args:\n",
        "        model (GPT): The GPT model to be trained.\n",
        "        train_dataset (Dataset): The dataset containing training examples.\n",
        "\n",
        "    Returns:\n",
        "        device (str): The device (CPU or GPU) used for training.\n",
        "    \"\"\"\n",
        "    # Get default training configuration\n",
        "    train_config = Trainer.get_default_config()\n",
        "    # Set learning rate for training\n",
        "    train_config.learning_rate = LEARNING_RATE\n",
        "    # Set maximum number of training iterations\n",
        "    train_config.max_iters = MAX_ITERS\n",
        "    # Set batch size for training\n",
        "    train_config.batch_size = BATCH_SIZE\n",
        "    # Initialize trainer with the specified configuration, model, and dataset\n",
        "    trainer = Trainer(train_config, model, train_dataset)\n",
        "    # Set callback function to monitor training progress\n",
        "    trainer.set_callback('on_batch_end', batch_end_callback)\n",
        "    # Start training process\n",
        "    trainer.run()\n",
        "    # Initialize variables for tracking training loss and time\n",
        "    trainer.init_time = None\n",
        "    trainer.train_loss_vals = []\n",
        "    trainer.train_loss_times = []\n",
        "    # Run training process again\n",
        "    trainer.run()\n",
        "    # Set the model to evaluation mode after training\n",
        "    model.eval()\n",
        "    # Return the device used for training\n",
        "    return trainer.device\n",
        "\n",
        "\n",
        "mini_gpt = load_model()\n",
        "train_device = train_model(mini_gpt, dataset)\n",
        "\n"
      ],
      "metadata": {
        "id": "HeoFv9gXa6pp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70e5bfd-b1b7-4489-9113-229a01656a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 123.68M\n",
            "running on device cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter_dt 0.00ms; iter 0: train loss 10.91157\n",
            "iter_dt 293.77ms; iter 100: train loss 5.09858\n",
            "iter_dt 308.88ms; iter 200: train loss 4.66978\n",
            "iter_dt 334.16ms; iter 300: train loss 4.48186\n",
            "iter_dt 324.31ms; iter 400: train loss 3.78834\n",
            "iter_dt 320.70ms; iter 500: train loss 3.92948\n",
            "iter_dt 357.58ms; iter 600: train loss 3.16907\n",
            "iter_dt 323.99ms; iter 700: train loss 3.61388\n",
            "iter_dt 325.01ms; iter 800: train loss 2.89561\n",
            "iter_dt 325.95ms; iter 900: train loss 2.86004\n",
            "iter_dt 336.38ms; iter 1000: train loss 2.73472\n",
            "iter_dt 328.73ms; iter 1100: train loss 2.37208\n",
            "iter_dt 314.89ms; iter 1200: train loss 2.44860\n",
            "iter_dt 335.84ms; iter 1300: train loss 2.27997\n",
            "iter_dt 315.60ms; iter 1400: train loss 1.94524\n",
            "iter_dt 326.44ms; iter 1500: train loss 1.78241\n",
            "iter_dt 326.62ms; iter 1600: train loss 1.61986\n",
            "iter_dt 326.55ms; iter 1700: train loss 1.49093\n",
            "iter_dt 296.66ms; iter 1800: train loss 1.23891\n",
            "iter_dt 322.92ms; iter 1900: train loss 1.05599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter_dt 304.23ms; iter 0: train loss 1.00131\n",
            "iter_dt 401.87ms; iter 100: train loss 0.92797\n",
            "iter_dt 252.33ms; iter 200: train loss 0.76371\n",
            "iter_dt 323.08ms; iter 300: train loss 0.81715\n",
            "iter_dt 297.52ms; iter 400: train loss 0.66154\n",
            "iter_dt 325.12ms; iter 500: train loss 0.71316\n",
            "iter_dt 325.33ms; iter 600: train loss 0.62251\n",
            "iter_dt 325.79ms; iter 700: train loss 0.59499\n",
            "iter_dt 325.54ms; iter 800: train loss 0.49902\n",
            "iter_dt 324.62ms; iter 900: train loss 0.53952\n",
            "iter_dt 325.70ms; iter 1000: train loss 0.47086\n",
            "iter_dt 327.06ms; iter 1100: train loss 0.54779\n",
            "iter_dt 325.21ms; iter 1200: train loss 0.43279\n",
            "iter_dt 326.11ms; iter 1300: train loss 0.48745\n",
            "iter_dt 326.79ms; iter 1400: train loss 0.46009\n",
            "iter_dt 326.16ms; iter 1500: train loss 0.49696\n",
            "iter_dt 326.30ms; iter 1600: train loss 0.44778\n",
            "iter_dt 321.00ms; iter 1700: train loss 0.39358\n",
            "iter_dt 325.81ms; iter 1800: train loss 0.42523\n",
            "iter_dt 322.22ms; iter 1900: train loss 0.43914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You now have a trained LM - congratulations! ðŸŽ‰ðŸŽ‰ðŸŽ‰\n",
        "\n",
        "\n",
        "It's time to put it to the test and generate some text (don't expect any miracles here, all it knows is \"Alice's Adventures in Wonderland\", so cut it some slack).\n",
        "\n",
        "You can experiment with different prompts, number of generated tokens, etc.\n"
      ],
      "metadata": {
        "id": "7t6ouJHeIgwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do so you will need to use the in-built in function 'generate' of the model (model.generate()), which receives as input:\n",
        "\n",
        "1. A torch.tensor of dtype=torch.long with the indexes of the tokens in the prompt - just like in training, start with a string prompt of your choosing, encode it using the encoder and convert the result to torch.tensor after wrapping it in list using torch.tensor([encoder_output], dtype=torch.long)\n",
        "\n",
        "**Note**: very technical but to avoid device error, convert the resulting tensor (t) to the device of the trainer using t.to(device = trainer.device)\n",
        "\n",
        "2. The number of words to generate - you can choose whatever positive number you like.\n",
        "\n",
        "\n",
        "The function returns token indexes, which you can covert to tokens/words using `encoder.decode(token_indexes.squeeze().tolist())`"
      ],
      "metadata": {
        "id": "xUdgf8i8dO0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, device, encoder, prompt, new_token_length):\n",
        "  initial_idx = torch.tensor([encoder.encode(prompt)], dtype = torch.long).to(device=device)\n",
        "  tokenized_output = model.generate(initial_idx, max_new_tokens=new_token_length - init_idx.shape[1], do_sample=False, top_k=40)\n",
        "  return encoder.decode(tokenized_output.squeeze().tolist())\n",
        "\n",
        "generate_text(mini_gpt, train_device, encoder, \"how is the queen doing?\" 20)\n"
      ],
      "metadata": {
        "id": "k0Cwpc3JbCRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the contextual embedding your model uses to generate text, we will take a closer look at the self-attention output.\n",
        "\n",
        "Generate a sentence with at least 10 tokens.\n",
        "In the last word prediction extract the attention scores\n",
        "from the **first** and **last** transformer blocks.\n",
        "Average the scores over the different attention heads."
      ],
      "metadata": {
        "id": "9C7XMoMuJ3sO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To extract the (averaged) attention scores you will need to modify your local **model.py** file at the following points in the code:\n",
        "1. Return the average attention scores in the *forward* function of the **CausalSelfAttention** class (apply the mean after the softmax before the dropout) as an additional output.\n",
        "2. Handle this additional output in the *forward* function of the **Block** class, and return it as well.\n",
        "3. In the *forward* function of the **GPT** class, make sure to save the attention scores from the relevant blocks and return them as a list along side the existing outputs.\n",
        "\n",
        "\n",
        "Or you can take the modified file from moodle to run the next section of code (replacing the model.py file you already have under mingpt directory).\n",
        "\n",
        "IMPORTANT NOTE:\n",
        "After modifying the model.py file you will need to restart your runtime and re-train your model.\n",
        "To do so change line 93 in trainer.py to be (restart runtime after changing this line):\n",
        "\n",
        "```\n",
        "logits, self.loss, _ = model(x, y)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "eb7d5y_5PERv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the modified model.py you can generate tokens using the in-build model.forward() function.\n",
        "Make sure to call `with torch.no_grad():` before the forward function.\n",
        "\n",
        "This function expects two inputs: torch.tensor of tokens indexes (like before), and attn_block_nums which is a list of blocks from which you would like to extract the attention.\n",
        "\n",
        "So an example of a call would be: `model.forward(tokens, attn_block_nums=[0, 3])`"
      ],
      "metadata": {
        "id": "fznyPs1LnJ98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ATTENTION_PROMPT = \"The King and Queen of Hearts were seated on their\"\n",
        "\n",
        "prompt_token_idxs = encoder.encode(ATTENTION_PROMPT)\n",
        "prompt_token_idxs_tensor = torch.tensor([prompt_token_idxs], dtype=torch.long).to(device=device)\n",
        "\n",
        "with torch.no_grad():\n",
        "      logits, _, attn_scores = model(prompt_token_idxs_tensor, attn_block_nums=[0, NUM_BLOCKS_IN_GPT - 1])"
      ],
      "metadata": {
        "id": "8bLMNKu4bUMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function returns three values: logits, loss, attns.\n",
        "\n",
        "You can ignore the loss, and using the function `logits_to_next_token_idx` get the token index of the word predicted/generated by the model.\n",
        "\n",
        "Then you can decode this token index using encoder.decode() to get the actual token and print it out.\n"
      ],
      "metadata": {
        "id": "fU2Rd-mURVfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logits_to_token_idx(logits):\n",
        "    next_token_logits = logits[:, -1, :]\n",
        "    probs = F.softmax(next_token_logits, dim=-1)\n",
        "    idx = torch.topk(probs, k=1)[1]\n",
        "    return idx.cpu().tolist()[0]"
      ],
      "metadata": {
        "id": "voXEDFL9WhZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next_token_index = logits_to_token_idx(logits)\n",
        "netx_token = encoder.decode(next_token_index)\n",
        "\n",
        "print(f\"Sentence sampled for attention: \\n\" + ATTENTION_PROMPT + netx_token)"
      ],
      "metadata": {
        "id": "HGdckGPVS46S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "attns as returned by the forward function is a tuple of (first_block_att_scores, last_block_att_scores).\n",
        "\n",
        "This is the attention score for all tokens predicted by the model, so you will need to use `first_block_att_scores.squeeze()[-1]` (and the same for last_block_att_scores) to get the attention for the last predicted token.\n",
        "\n",
        "So you need to split it, and then pass each of those to the function \"paint_tokens_by_attention\" function, to paint the tokens by their attention weight.\n",
        "\n",
        "Remember to pass tokens not the original prompt to this painting function.\n",
        "\n",
        "To get them you can encode the prompt with the encoder, and then decode each item in the list separately wrapped in a list (the decode() function accepts only lists)."
      ],
      "metadata": {
        "id": "Leifa6CmS3MM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "def paint_tokens_by_attention(tokens, attention_scores):\n",
        "    plt.cla()\n",
        "    attn = [attention_scores.cpu().numpy().round(3)]\n",
        "    plt.table(cellText=attn,\n",
        "              colLabels=tokens,\n",
        "              loc='center',\n",
        "              cellColours=cm.Oranges(attn))\n",
        "    plt.axis('off')\n",
        "    plt.rcParams['figure.dpi'] = 300\n",
        "    plt.rcParams['savefig.dpi'] = 300\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "d-IDSnyySgEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_block_att_scores, last_block_att_scores = attn_scores\n",
        "first_block_att_scores = first_block_att_scores.squeeze()[-1]\n",
        "last_block_att_scores = last_block_att_scores.squeeze()[-1]\n",
        "\n",
        "prompt_tokens = [encoder.decode([token_idx]) for token_idx in prompt_token_idxs]\n",
        "\n",
        "paint_tokens_by_attention(prompt_tokens, first_block_att_scores)\n",
        "paint_tokens_by_attention(prompt_tokens, last_block_att_scores)"
      ],
      "metadata": {
        "id": "oMkc3WJvSxnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pay attention to the following:\n",
        "1. What is the sum of all attention scores?\n",
        "2. What is the difference in attention between the blocks?\n",
        "3. Based on this analysis why do you think the model \"chose\" the predicted word/token?"
      ],
      "metadata": {
        "id": "bTBUWKDcO9J8"
      }
    }
  ]
}